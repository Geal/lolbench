{% extends "base.html" %}

{% block title %}home{% endblock %}

{% block head %}
<style>
</style>
{% endblock %}

{% block content %}
<h1>lolbench home</h1>

<b><a href="#recent-changes">potentially anomalous results</a> | <a href="#all-benchmarks">all benchmarks</a></b>

<h2>about</h2>

<p>benchmarks of binaries generated by rustc</p>
<p><a href="https://github.com/anp/lolbench">project on github</a></p>
<p>
    most benchmark suites track the performance of a program as the implementation changes. lolbench
    does this as well, but the only implementation detail that's allowed to change is the version
    of the rust toolchain used. currently runs benchmarks with each nightly.
</p>

<h2><a id="recent-changes" />potentially anomalous results</h2>

<p>
    each entry here passes some statistical tests for being "interesting" results, but has not
    necessarily been manually reproduced and confirmed as a regression or improvement. in the near
    future we hope to allow marking these anomalous results as "resolved" and associating them with
    commit and/or issue metadata. see
    <a target="_blank" href="https://github.com/anp/lolbench/issues/14">the relevant github issue</a>
    if you'd like to help with that!
</p>

{% for anomaly in analysis.anomalous_timings %}
<h4>toolchain: {{ anomaly.0 }}</h4>

{% for noteworthy in anomaly.1 %}
<p>
    {{ noteworthy.index.nanoseconds }}
    {{ noteworthy.benchmark_for_linking().link()|safe }}
</p>
{% endfor %}
{% endfor %}

<h2><a id="all-benchmarks" />all benchmarks</h2>

<table border="1">
    <tbody>
        {% for benchmark in benchmarks %}
        <tr>
            <td>
                {{benchmark.link()|safe}}
            </td>
        </tr>
        {% endfor %}
    </tbody>
</table>

{% endblock %}
